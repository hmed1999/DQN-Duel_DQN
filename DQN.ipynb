{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P4ibNEnrnfZ-"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size, input_shape, n_actions, discrete=False):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "        self.discrete = discrete\n",
        "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
        "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
        "        dtype = np.int8 if self.discrete else np.float32\n",
        "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=dtype)\n",
        "        self.reward_memory = np.zeros(self.mem_size)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        # store one hot encoding of actions, if appropriate\n",
        "        if self.discrete:\n",
        "            actions = np.zeros(self.action_memory.shape[1])\n",
        "            actions[action] = 1.0\n",
        "            self.action_memory[index] = actions\n",
        "        else:\n",
        "            self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = 1 - done\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "        batch = np.random.choice(max_mem, batch_size)\n",
        "\n",
        "        states = self.state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        terminal = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, states_, terminal"
      ],
      "metadata": {
        "id": "MlLrmmAnnj9E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dqn(lr, n_actions, input_dims, fc1_dims, fc2_dims):\n",
        "    model = Sequential([\n",
        "                Dense(fc1_dims, input_shape=(input_dims,)),\n",
        "                Activation('relu'),\n",
        "                Dense(fc2_dims),\n",
        "                Activation('relu'),\n",
        "                Dense(n_actions)])\n",
        "\n",
        "    model.compile(optimizer=Adam(lr=lr), loss='mse')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "yz2mi805nnlE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent(object):\n",
        "    def __init__(self, alpha, gamma, n_actions, epsilon, batch_size,\n",
        "                 input_dims, epsilon_dec=0.996,  epsilon_end=0.01,\n",
        "                 number_steps_to_update_target_network=100,mem_size=1000000, fname='dqn_model.h5'):\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_dec = epsilon_dec\n",
        "        self.epsilon_min = epsilon_end\n",
        "        self.batch_size = batch_size\n",
        "        self.model_file = fname\n",
        "        self.memory = ReplayBuffer(mem_size, input_dims, n_actions,\n",
        "                                   discrete=True)\n",
        "        self.q_eval = build_dqn(alpha, n_actions, input_dims, 256, 256)\n",
        "        self.q_target = self.q_eval\n",
        "        self.number_steps_to_update_target_network = number_steps_to_update_target_network\n",
        "        self.step_count_number=0\n",
        "\n",
        "    def remember(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        state = state[np.newaxis, :]\n",
        "        rand = np.random.random()\n",
        "        if rand < self.epsilon:\n",
        "            action = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            actions = self.q_eval.predict(state)\n",
        "            action = np.argmax(actions)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_cntr > self.batch_size:\n",
        "            state, action, reward, new_state, done = \\\n",
        "                                          self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "            action_values = np.array(self.action_space, dtype=np.int8)\n",
        "            action_indices = np.dot(action, action_values)\n",
        "\n",
        "            q_eval = self.q_eval.predict(state)\n",
        "\n",
        "            q_next = self.q_target.predict(new_state)\n",
        "\n",
        "            q_target = q_eval.copy()\n",
        "\n",
        "            batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "\n",
        "            q_target[batch_index, action_indices] = reward + \\\n",
        "                                  self.gamma*np.max(q_next, axis=1)*done\n",
        "\n",
        "            _ = self.q_eval.fit(state, q_target, verbose=0)\n",
        "            self.step_count_number+=1\n",
        "            if(self.step_count_number == self.number_steps_to_update_target_network):\n",
        "              self.q_target = self.q_eval\n",
        "              self.step_count_number = 0\n",
        "            self.epsilon = self.epsilon*self.epsilon_dec if self.epsilon > \\\n",
        "                           self.epsilon_min else self.epsilon_min\n",
        "\n",
        "    def save_model(self):\n",
        "        self.q_eval.save(self.model_file)\n",
        "\n",
        "    def load_model(self):\n",
        "        self.q_eval = load_model(self.model_file)"
      ],
      "metadata": {
        "id": "NRmkAr3inr56"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotLearning(x, scores, epsilons, filename, lines=None):\n",
        "    fig=plt.figure()\n",
        "    ax=fig.add_subplot(111, label=\"1\")\n",
        "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
        "\n",
        "    ax.plot(x, epsilons, color=\"C0\")\n",
        "    ax.set_xlabel(\"Game\", color=\"C0\")\n",
        "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
        "    ax.tick_params(axis='x', colors=\"C0\")\n",
        "    ax.tick_params(axis='y', colors=\"C0\")\n",
        "\n",
        "    N = len(scores)\n",
        "    running_avg = np.empty(N)\n",
        "    for t in range(N):\n",
        "\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
        "\n",
        "    ax2.scatter(x, running_avg, color=\"C1\")\n",
        "    #ax2.xaxis.tick_top()\n",
        "    ax2.axes.get_xaxis().set_visible(False)\n",
        "    ax2.yaxis.tick_right()\n",
        "    #ax2.set_xlabel('x label 2', color=\"C1\")\n",
        "    ax2.set_ylabel('Score', color=\"C1\")\n",
        "    #ax2.xaxis.set_label_position('top')\n",
        "    ax2.yaxis.set_label_position('right')\n",
        "    #ax2.tick_params(axis='x', colors=\"C1\")\n",
        "    ax2.tick_params(axis='y', colors=\"C1\")\n",
        "\n",
        "    if lines is not None:\n",
        "        for line in lines:\n",
        "            plt.axvline(x=line)\n",
        "\n",
        "    plt.savefig(filename)"
      ],
      "metadata": {
        "id": "j7KtAVEKnwD4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]\n",
        "import gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVY94KYob3rL",
        "outputId": "5ec44674-30f1-4cca-87a2-7b49a89de33e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    env = gym.make('LunarLander-v2')\n",
        "    lr = 0.0005\n",
        "    n_games = 500\n",
        "    agent = Agent(gamma=0.99, epsilon=1.0, alpha=lr, input_dims=8,\n",
        "                  n_actions=4, mem_size=1000000, batch_size=64, epsilon_end=0.01)\n",
        "\n",
        "    #agent.load_model()\n",
        "    scores = []\n",
        "    eps_history = []\n",
        "\n",
        "    #env = wrappers.Monitor(env, \"tmp/lunar-lander-6\",\n",
        "    #                         video_callable=lambda episode_id: True, force=True)\n",
        "\n",
        "    for i in range(n_games):\n",
        "        done = False\n",
        "        score = 0\n",
        "        observation = env.reset()\n",
        "        while not done:\n",
        "            action = agent.choose_action(observation)\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            agent.remember(observation, action, reward, observation_, int(done))\n",
        "            observation = observation_\n",
        "            agent.learn()\n",
        "\n",
        "        eps_history.append(agent.epsilon)\n",
        "        scores.append(score)\n",
        "\n",
        "        avg_score = np.mean(scores[max(0, i-100):(i+1)])\n",
        "        print('episode: ', i,'score: %.2f' % score,\n",
        "              ' average score %.2f' % avg_score)\n",
        "\n",
        "        if i % 10 == 0 and i > 0:\n",
        "            agent.save_model()\n",
        "\n",
        "    filename = 'lunarlander.png'\n",
        "\n",
        "    x = [i+1 for i in range(n_games)]\n",
        "    plotLearning(x, scores, eps_history, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmKn6T3fnzUc",
        "outputId": "74afed4e-93da-4c4b-f7f0-5c596edcbc05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode:  0 score: -199.83  average score -199.83\n",
            "episode:  1 score: -254.13  average score -226.98\n",
            "episode:  2 score: -176.19  average score -210.05\n",
            "episode:  3 score: -196.04  average score -206.55\n",
            "episode:  4 score: -229.85  average score -211.21\n",
            "episode:  5 score: -55.26  average score -185.22\n",
            "episode:  6 score: -64.84  average score -168.02\n",
            "episode:  7 score: -160.42  average score -167.07\n",
            "episode:  8 score: -183.79  average score -168.93\n",
            "episode:  9 score: -196.06  average score -171.64\n",
            "episode:  10 score: -159.29  average score -170.52\n",
            "episode:  11 score: -221.24  average score -174.74\n",
            "episode:  12 score: -214.88  average score -177.83\n",
            "episode:  13 score: -306.34  average score -187.01\n"
          ]
        }
      ]
    }
  ]
}